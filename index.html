<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="##########">
    <meta name="keywords" content="LatentCLR">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions</title>
    <!-- BAAk -->
    <!-- Global site tag (gtag.js) - Google Analytics  -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');


    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>
<!-- BAAAAAAAAAAAAAAAAAAAAAK -->
<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
            </a>

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://hypernerf.github.io">
                        HyperNeRF
                    </a>
                    <a class="navbar-item" href="https://nerfies.github.io">
                        Nerfies
                    </a>
                    <a class="navbar-item" href="https://latentfusion.github.io">
                        LatentFusion
                    </a>
                    <a class="navbar-item" href="https://photoshape.github.io">
                        PhotoShape
                    </a>
                </div>
            </div>
        </div>

    </div>
</nav>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">LatentCLR: A Contrastive Learning Approach for Unsupervised
                        Discovery of Interpretable Directions</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://.com">Oguz Kaan Yuksel</a><sup>1</sup>,</span>
                        <span class="author-block">
              <a href="https://.com">Enis Simsar</a><sup>2,3</sup>,</span>
                        <span class="author-block">
              <a href="https://.info">Ezgi Gulperi Er</a><sup>3</sup>,
            </span>
                        <span class="author-block">
              <a href="https://pinguar.org/">Pinar Yanardag</a><sup>3</sup>,
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>EPFL,</span>
                        <span class="author-block"><sup>2</sup>TUM,</span>
                        <span class="author-block"><sup>3</sup>Bogazici University</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href="https://arxiv.org/pdf/2104.00820.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2104.00820"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Video Link. ADDDDDDDDDDDDDDDDDDD -->
                            <span class="link-block">
                <a href=".com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a href="https://github.com/catlab-team/latentclr"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <video id="teaser" autoplay muted loop height="100%">
                <source src=".com"
                        type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
                <span class="dnerf">LatentCLR</span> add video!
            </h2>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">

                    <p>
                        Recent research has shown that it is possible to find interpretable
                        directions in the latent spaces of pre-trained
                        Generative Adversarial Networks (GANs). These directions
                        enable controllable image generation and support a wide
                        range of semantic editing operations, such as zoom or rotation.
                        The discovery of such directions is often done in a
                        supervised or semi-supervised manner and requires manual
                        annotations which limits their use in practice. In comparison,
                        unsupervised discovery allows finding subtle directions
                        that are difficult to detect a priori.
                    </p>
                    <p>
                        In this work, we propose a contrastive learning-based approach to discover semantic directions
                        in the latent space of
                        pre-trained GANs in a selfsupervised
                        manner. Our approach finds semantically meaningful
                        dimensions compatible with state-of-the-art methods.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-steve">
                        <img src="static/images/teaser_biggan1.png">
                    </div>
                    <div class="item item-chair-tp">
                        <img src="static/images/teaser_biggan2.png">
                    </div>
                    <div class="item item-shiba">
                        <img src="static/images/teaser_car.png">
                    </div>
                    <div class="item item-fullbody">
                        <img src="static/images/teaser_cat.png">
                    </div>
                    <div class="item item-blueshirt">
                        <img src="static/images/teaser_ffhq.png">

                    </div>
                    <div class="item item-mask">
                        <img src="static/images/teaser_ffhq_boy.png">

                    </div>

                </div>
            </div>
        </div>
        <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">StyleGAN2 Directions</h2>
                <div class="publication-video">
                    <img src="static/images/stylegan_ours.png" height="940">
                </div>
                <div class="publication-video">
                    <img src="static/images/stylegan_others.png">
                </div>
            </div>
        </div>

        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">BigGAN Directions - Class Specific</h2>
                <div class="publication-video">
                    <img src="static/images/class_specific.png">
                </div>
            </div>
        </div>

    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Re-rendering. -->
        <h2 class="title is-3">Transferability of directions</h2>
        <div class="content has-text-justified">
            <p>
                Our visual analysis shows that the directions learned
                from the one ImageNet class are applicable to a variety of ImageNet
                classes
            </p>
        </div>
        <div class="content has-text-centered">

        </div>
        <div class="columns is-centered">

            <!-- Visual Effects. -->
            <div class="column">
                <div class="content">
                    <h3 class="title is-4">Zoom</h3>
                    <p>
                        Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
                        would be impossible without nerfies since it would require going through a wall.
                    </p>
                    <video id="dollyzoom" autoplay controls muted loop height="100%">
                        <source src="https://imgflip.com/gif/5o1emn"
                                type="video/mp4">
                    </video>
                </div>
            </div>
            <!--/ Visual Effects. -->

            <!-- Matting. -->
            <div class="column">
                <h3 class="title is-4">Rotate</h3>
                <div class="columns is-centered">
                    <div class="column content">
                        <p>
                            As a byproduct of our method, we can also solve the matting problem by ignoring
                            samples that fall outside of a bounding box during rendering.
                        </p>
                        <video id="matting-video" controls height="100%">
                            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/matting.mp4"
                                    type="video/mp4">
                        </video>
                    </div>

                </div>
            </div>
        </div>
        <!--/ Matting. -->
        <div class="columns is-centered">

            <!-- Visual Effects. -->
            <div class="column">
                <div class="content">
                    <h3 class="title is-4">Contrast</h3>
                    <p>
                        Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
                        would be impossible without nerfies since it would require going through a wall.
                    </p>
                    <video id="dollyzoom" autoplay controls muted loop height="100%">
                        <source src="https://imgflip.com/gif/5o1emn"
                                type="video/mp4">
                    </video>
                </div>
            </div>
            <!--/ Visual Effects. -->

            <!-- Matting. -->
            <div class="column">
                <h3 class="title is-4">Sitting</h3>
                <div class="columns is-centered">
                    <div class="column content">
                        <p>
                            As a byproduct of our method, we can also solve the matting problem by ignoring
                            samples that fall outside of a bounding box during rendering.
                        </p>
                        <video id="matting-video" controls height="100%">
                            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/matting.mp4"
                                    type="video/mp4">
                        </video>
                    </div>

                </div>
            </div>
        </div>
        <!-- Animation. -->
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">Diversity of the directions</h2>

                <div class="content has-text-justified">
                    <p>
                        We can also animate the scene by interpolating the deformation latent codes of two input
                        frames. Use the slider here to linearly interpolate between the left frame and the right
                        frame.
                    </p>
                </div>
                <div class="columns is-vcentered interpolation-panel">
                    <div class="column is-3 has-text-centered">
                        <img src="https://homes.cs.washington.edu/~kpar/nerfies/images/interpolate_start.jpg"
                             class="interpolation-image"
                             alt="Interpolate start reference image."/>
                        <p>Start Frame</p>
                    </div>
                    <div class="column interpolation-video-column">
                        <div id="interpolation-image-wrapper">
                            Loading...
                        </div>
                        <input class="slider is-fullwidth is-large is-info"
                               id="interpolation-slider"
                               step="1" min="0" max="100" value="0" type="range">
                    </div>
                    <div class="column is-3 has-text-centered">
                        <img src="https://homes.cs.washington.edu/~kpar/nerfies/images/interpolate_end.jpg"
                             class="interpolation-image"
                             alt="Interpolation end reference image."/>
                        <p class="is-bold">End Frame</p>
                    </div>
                </div>
                <br/>
                <!--/ Interpolating. -->


                <!--/ Re-rendering. -->

            </div>
        </div>
        <!--/ Animation. -->


        <!-- Concurrent Work. -->
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">LatentCLR Framework</h2>

                <div class="content has-text-justified">
                    <img src="static/images/LatentCLR.png">
                    <p>
                        Each latent code is passed through direction models and up to
                        a target feature layer of the GAN to obtain intermediate representations of
                        edited codes. Then, the effects
                        of direction models are computed by subtracting the representation of the original latent code.
                        Finally, pairs produced by the
                        same model are considered as positive, and others as negative, in a contrastive loss.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Concurrent Work. -->

        <div class="container is-max-desktop">
            <!-- Re-rendering. -->
            <h2 class="title is-3">Comparison with other methods</h2>
            <div class="content has-text-justified">
                <p>
                    We compare how the
                    directions found on FFHQ differ across methods. Figure shows the visual comparison between several
                    directions found in common by all methods, including the directions
                    Smile, Lipstick, Elderly, Curly Hair , and Young.
                </p>
                <img src="static/images/full_comparison.png">
            </div>

        </div>
    </div>

</section>


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{DBLP:journals/corr/abs-2104-00820,
  author    = {Oguz Kaan Y{\"{u}}ksel and
               Enis Simsar and
               Ezgi G{\"{u}}lperi Er and
               Pinar Yanardag},
  title     = {LatentCLR: {A} Contrastive Learning Approach for Unsupervised Discovery
               of Interpretable Directions},
  journal   = {CoRR},
  volume    = {abs/2104.00820},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.00820},
  eprinttype = {arXiv},
  eprint    = {2104.00820},
  timestamp = {Mon, 12 Apr 2021 16:14:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-00820.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <a class="icon-link"
               href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
                <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
                <i class="fab fa-github"></i>
            </a>
        </div>
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Template from <a
                            href="https://github.com/nerfies/nerfies.github.io">nerfies</a> o
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
